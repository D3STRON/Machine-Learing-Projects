{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "17e6cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import unicodedata\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd885a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./data/train.csv\")\n",
    "test_df = pd.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "4da2ebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim= 1)\n",
    "        \n",
    "    def forward(self, input_tensor, hidden_tensor):\n",
    "        combined = torch.cat((input_tensor, hidden_tensor), 1)\n",
    "#         print(combined)\n",
    "        hidden  = self.i2h(combined)\n",
    "        output = self.softmax(self.i2o(combined))\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "1d268d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'three people died from the heat wave so far'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df[\"target\"] == 1][\"text\"].values[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "6f9bb23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 22328)\n",
      "[[0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "\n",
    "## let's get counts for the first 5 tweets in the data\n",
    "example_train_vectors = count_vectorizer.fit_transform(train_df[\"text\"][0:len(train_df)])\n",
    "\n",
    "print(example_train_vectors[0].todense().shape)\n",
    "print(example_train_vectors[0].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "52cd7814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  our deeds are the reason of this earthquake ma...   \n",
       "1   4     NaN      NaN              forest fire near la ronge sask canada   \n",
       "2   5     NaN      NaN  all residents asked to shelter in place are be...   \n",
       "3   6     NaN      NaN  13000 people receive wildfires evacuation orde...   \n",
       "4   7     NaN      NaN  just got sent this photo from ruby alaska as s...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "910aeb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df['keyword']\n",
    "del train_df['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "0c3d5ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,;'\n",
      "Slusarski\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.]])\n",
      "tensor(0)\n",
      "torch.Size([85, 1, 57])\n"
     ]
    }
   ],
   "source": [
    "ALL_LETTERS = string.ascii_letters + \" .,;'\"\n",
    "N_LETTERS = len(ALL_LETTERS)\n",
    "\n",
    "def text_cleaner(text):\n",
    "    text = text.lower() # convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text) # remove punctuation and special characters\n",
    "    text = re.sub(r'\\s+', ' ', text) # remove extra whitespace\n",
    "    text = re.sub(r'https?://\\S+', '', text) # remove URLs\n",
    "    text = re.sub(r\"#\", \"\", text)\n",
    "    return text\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in ALL_LETTERS\n",
    "    )\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letter_to_index(letter):\n",
    "    return ALL_LETTERS.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letter_to_tensor(letter):\n",
    "    tensor = torch.zeros(1, N_LETTERS)\n",
    "    tensor[0][letter_to_index(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def line_to_tensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, N_LETTERS)\n",
    "    for i, letter in enumerate(line):\n",
    "        tensor[i][0][letter_to_index(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "def random_training_exmple(data_frame):\n",
    "    example = data_frame.sample()\n",
    "    return line_to_tensor(example[\"text\"].to_numpy()[0]), torch.tensor(example[\"target\"].to_numpy())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(ALL_LETTERS)\n",
    "    print(unicode_to_ascii('Ślusàrski'))\n",
    "    \n",
    "    example_text, target = random_training_exmple(train_df)\n",
    "    print(example_text)\n",
    "    \n",
    "    print(letter_to_tensor('J')) # [1, 57]\n",
    "    print(target[0])\n",
    "    print(example_text.size()) # [5, 1, 57]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "e9e7856c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "train_df['text'] = train_df['text'].apply(text_cleaner)\n",
    "\n",
    "n_hidden = 128\n",
    "rnn = RNN(N_LETTERS, n_hidden, 2)\n",
    "\n",
    "input_tensor = letter_to_tensor('A')\n",
    "hidden_tensor = rnn.init_hidden()\n",
    "\n",
    "# implemeting one step\n",
    "output, next_hidden = rnn(input_tensor, hidden_tensor)\n",
    "print(output.size())\n",
    "print(next_hidden.size())\n",
    "\n",
    "input_tensor2 = line_to_tensor('Albert')\n",
    "output, next_hidden = rnn(input_tensor2[0], hidden_tensor)\n",
    "print(output.size())\n",
    "print(next_hidden.size())\n",
    "\n",
    "# Learning loop\n",
    "criterion = nn.NLLLoss()\n",
    "learning_rate = 0.005\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "9bf9dbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_from_output(output):\n",
    "    category_index = torch.argmax(output).item()\n",
    "    return category_index\n",
    "\n",
    "\n",
    "def train(line_tensor, category_tensor):\n",
    "    hidden = rnn.init_hidden()\n",
    "    \n",
    "    # goes eover every charater in the name and keep feeding it to the nn\n",
    "    # that and the hidden state fom before\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "    loss = criterion(output, category_tensor)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return output, loss.item()\n",
    "\n",
    "\n",
    "\n",
    "def predict(input_line):\n",
    "    print(\"\\n> {input_line}\")\n",
    "    with torch.no_grad():\n",
    "        line_tensor = line_to_tensor(input_line)\n",
    "        hidden = rnn.init_hidden()\n",
    "        # goes eover every charater in the name and keep feeding it to the nn\n",
    "        # that and the hidden state fom before\n",
    "        for i in range(line_tensor.size()[0]):\n",
    "            output, hidden = rnn(line_tensor[i], hidden)\n",
    "        guess = category_from_output(output)\n",
    "        print(guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "c4bcf448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.2 0.7204 / 0 Wrong\n",
      "200 0.4 0.8420 / 0 Wrong\n",
      "300 0.6 0.4954 / 0 Correct\n",
      "400 0.8 0.5400 / 0 Correct\n",
      "500 1.0 0.9513 / 0 Wrong\n",
      "600 1.2 0.4849 / 0 Correct\n",
      "700 1.4000000000000001 0.4613 / 0 Correct\n",
      "800 1.6 0.4642 / 0 Correct\n",
      "900 1.7999999999999998 0.5311 / 0 Correct\n",
      "1000 2.0 0.5161 / 0 Correct\n",
      "1100 2.1999999999999997 0.4678 / 0 Correct\n",
      "1200 2.4 0.8252 / 0 Wrong\n",
      "1300 2.6 0.5446 / 0 Correct\n",
      "1400 2.8000000000000003 0.7425 / 0 Wrong\n",
      "1500 3.0 0.5983 / 0 Correct\n",
      "1600 3.2 0.7407 / 0 Wrong\n",
      "1700 3.4000000000000004 0.5500 / 0 Correct\n",
      "1800 3.5999999999999996 0.6093 / 0 Correct\n",
      "1900 3.8 0.5919 / 0 Correct\n",
      "2000 4.0 0.7695 / 0 Wrong\n",
      "2100 4.2 0.7002 / 1 Wrong\n",
      "2200 4.3999999999999995 0.5418 / 0 Correct\n",
      "2300 4.6 0.4516 / 0 Correct\n",
      "2400 4.8 0.7145 / 0 Wrong\n",
      "2500 5.0 0.6552 / 0 Correct\n",
      "2600 5.2 0.7060 / 0 Wrong\n",
      "2700 5.4 0.7274 / 0 Wrong\n",
      "2800 5.6000000000000005 0.9259 / 0 Wrong\n",
      "2900 5.800000000000001 0.4927 / 0 Correct\n",
      "3000 6.0 0.6709 / 0 Correct\n",
      "3100 6.2 0.4801 / 0 Correct\n",
      "3200 6.4 0.7899 / 0 Wrong\n",
      "3300 6.6000000000000005 0.5585 / 0 Correct\n",
      "3400 6.800000000000001 0.7510 / 0 Wrong\n",
      "3500 7.000000000000001 0.8739 / 0 Wrong\n",
      "3600 7.199999999999999 0.5976 / 0 Correct\n",
      "3700 7.3999999999999995 0.6449 / 0 Correct\n",
      "3800 7.6 0.5935 / 0 Correct\n",
      "3900 7.8 0.5912 / 0 Correct\n",
      "4000 8.0 0.6017 / 0 Correct\n",
      "4100 8.200000000000001 0.5361 / 0 Correct\n",
      "4200 8.4 0.9292 / 0 Wrong\n",
      "4300 8.6 0.6536 / 0 Correct\n",
      "4400 8.799999999999999 0.7112 / 1 Wrong\n",
      "4500 9.0 0.7367 / 0 Wrong\n",
      "4600 9.2 1.0969 / 0 Wrong\n",
      "4700 9.4 0.6834 / 0 Correct\n",
      "4800 9.6 0.7256 / 0 Wrong\n",
      "4900 9.8 0.5798 / 1 Correct\n",
      "5000 10.0 0.7054 / 1 Wrong\n",
      "5100 10.2 0.4337 / 0 Correct\n",
      "5200 10.4 0.8103 / 1 Wrong\n",
      "5300 10.6 0.5613 / 0 Correct\n",
      "5400 10.8 0.6747 / 1 Correct\n",
      "5500 11.0 0.6507 / 0 Correct\n",
      "5600 11.200000000000001 0.7943 / 1 Wrong\n",
      "5700 11.4 0.5386 / 1 Correct\n",
      "5800 11.600000000000001 0.9370 / 0 Wrong\n",
      "5900 11.799999999999999 0.7029 / 1 Wrong\n",
      "6000 12.0 0.4507 / 0 Correct\n",
      "6100 12.2 0.7818 / 0 Wrong\n",
      "6200 12.4 0.4219 / 0 Correct\n",
      "6300 12.6 0.8198 / 0 Wrong\n",
      "6400 12.8 0.3408 / 0 Correct\n",
      "6500 13.0 0.5924 / 1 Correct\n",
      "6600 13.200000000000001 0.5923 / 0 Correct\n",
      "6700 13.4 0.6589 / 0 Correct\n",
      "6800 13.600000000000001 0.6640 / 0 Correct\n",
      "6900 13.8 0.5121 / 0 Correct\n",
      "7000 14.000000000000002 0.8451 / 1 Wrong\n",
      "7100 14.2 0.3861 / 0 Correct\n",
      "7200 14.399999999999999 0.4366 / 0 Correct\n",
      "7300 14.6 0.8452 / 0 Wrong\n",
      "7400 14.799999999999999 0.5988 / 0 Correct\n",
      "7500 15.0 0.4723 / 0 Correct\n",
      "7600 15.2 0.7368 / 0 Wrong\n",
      "7700 15.4 0.7653 / 1 Wrong\n",
      "7800 15.6 0.7496 / 0 Wrong\n",
      "7900 15.8 0.8516 / 0 Wrong\n",
      "8000 16.0 0.4726 / 0 Correct\n",
      "8100 16.2 0.8795 / 0 Wrong\n",
      "8200 16.400000000000002 0.3516 / 0 Correct\n",
      "8300 16.6 0.5881 / 0 Correct\n",
      "8400 16.8 0.3695 / 0 Correct\n",
      "8500 17.0 0.7125 / 1 Wrong\n",
      "8600 17.2 0.6133 / 0 Correct\n",
      "8700 17.4 0.6216 / 0 Correct\n",
      "8800 17.599999999999998 0.6062 / 0 Correct\n",
      "8900 17.8 0.9072 / 0 Wrong\n",
      "9000 18.0 0.7448 / 0 Wrong\n",
      "9100 18.2 0.7076 / 1 Wrong\n",
      "9200 18.4 1.0555 / 0 Wrong\n",
      "9300 18.6 0.8018 / 1 Wrong\n",
      "9400 18.8 0.4985 / 0 Correct\n",
      "9500 19.0 1.0000 / 0 Wrong\n",
      "9600 19.2 1.0179 / 0 Wrong\n",
      "9700 19.400000000000002 0.8825 / 0 Wrong\n",
      "9800 19.6 0.5933 / 0 Correct\n",
      "9900 19.8 0.7142 / 1 Wrong\n",
      "10000 20.0 1.2040 / 0 Wrong\n",
      "10100 20.200000000000003 0.7811 / 1 Wrong\n",
      "10200 20.4 0.6749 / 1 Correct\n",
      "10300 20.599999999999998 0.5954 / 1 Correct\n",
      "10400 20.8 0.6472 / 0 Correct\n",
      "10500 21.0 0.6191 / 0 Correct\n",
      "10600 21.2 0.7325 / 0 Wrong\n",
      "10700 21.4 0.3847 / 0 Correct\n",
      "10800 21.6 0.2698 / 0 Correct\n",
      "10900 21.8 0.5790 / 0 Correct\n",
      "11000 22.0 0.7078 / 1 Wrong\n",
      "11100 22.2 0.6007 / 0 Correct\n",
      "11200 22.400000000000002 0.2684 / 1 Correct\n",
      "11300 22.6 0.7969 / 1 Wrong\n",
      "11400 22.8 0.2496 / 0 Correct\n",
      "11500 23.0 0.6770 / 0 Correct\n",
      "11600 23.200000000000003 0.8160 / 0 Wrong\n",
      "11700 23.400000000000002 0.4039 / 0 Correct\n",
      "11800 23.599999999999998 0.9655 / 0 Wrong\n",
      "11900 23.799999999999997 0.8269 / 0 Wrong\n",
      "12000 24.0 0.6476 / 0 Correct\n",
      "12100 24.2 0.9122 / 0 Wrong\n",
      "12200 24.4 0.3420 / 0 Correct\n",
      "12300 24.6 0.5643 / 1 Correct\n",
      "12400 24.8 0.5448 / 1 Correct\n",
      "12500 25.0 1.1669 / 1 Wrong\n",
      "12600 25.2 0.6418 / 0 Correct\n",
      "12700 25.4 0.2673 / 0 Correct\n",
      "12800 25.6 0.8565 / 0 Wrong\n",
      "12900 25.8 0.6368 / 0 Correct\n",
      "13000 26.0 0.8378 / 0 Wrong\n",
      "13100 26.200000000000003 0.4981 / 0 Correct\n",
      "13200 26.400000000000002 0.7726 / 0 Wrong\n",
      "13300 26.6 0.5756 / 1 Correct\n",
      "13400 26.8 0.8562 / 1 Wrong\n",
      "13500 27.0 0.5169 / 0 Correct\n",
      "13600 27.200000000000003 0.4818 / 0 Correct\n",
      "13700 27.400000000000002 0.6681 / 0 Correct\n",
      "13800 27.6 0.6766 / 0 Correct\n",
      "13900 27.800000000000004 0.5299 / 0 Correct\n",
      "14000 28.000000000000004 0.4277 / 0 Correct\n",
      "14100 28.199999999999996 0.9595 / 0 Wrong\n",
      "14200 28.4 0.6613 / 0 Correct\n",
      "14300 28.599999999999998 0.6464 / 0 Correct\n",
      "14400 28.799999999999997 0.9919 / 0 Wrong\n",
      "14500 28.999999999999996 0.7480 / 0 Wrong\n",
      "14600 29.2 1.0058 / 1 Wrong\n",
      "14700 29.4 0.9162 / 0 Wrong\n",
      "14800 29.599999999999998 0.5393 / 1 Correct\n",
      "14900 29.799999999999997 0.7480 / 0 Wrong\n",
      "15000 30.0 0.5267 / 0 Correct\n",
      "15100 30.2 0.9445 / 0 Wrong\n",
      "15200 30.4 0.6274 / 1 Correct\n",
      "15300 30.599999999999998 0.2988 / 0 Correct\n",
      "15400 30.8 0.5396 / 0 Correct\n",
      "15500 31.0 0.4783 / 0 Correct\n",
      "15600 31.2 0.5393 / 0 Correct\n",
      "15700 31.4 0.7727 / 0 Wrong\n",
      "15800 31.6 0.4112 / 0 Correct\n",
      "15900 31.8 0.4663 / 0 Correct\n",
      "16000 32.0 0.5697 / 1 Correct\n",
      "16100 32.2 1.1492 / 0 Wrong\n",
      "16200 32.4 0.4262 / 0 Correct\n",
      "16300 32.6 0.8988 / 1 Wrong\n",
      "16400 32.800000000000004 0.7550 / 1 Wrong\n",
      "16500 33.0 0.9084 / 0 Wrong\n",
      "16600 33.2 0.7968 / 0 Wrong\n",
      "16700 33.4 0.7511 / 1 Wrong\n",
      "16800 33.6 0.4444 / 0 Correct\n",
      "16900 33.800000000000004 0.2715 / 0 Correct\n",
      "17000 34.0 0.8423 / 1 Wrong\n",
      "17100 34.2 0.6077 / 0 Correct\n",
      "17200 34.4 0.5755 / 0 Correct\n",
      "17300 34.599999999999994 1.0180 / 0 Wrong\n",
      "17400 34.8 0.5551 / 0 Correct\n",
      "17500 35.0 0.4292 / 0 Correct\n",
      "17600 35.199999999999996 0.4116 / 0 Correct\n",
      "17700 35.4 0.4564 / 0 Correct\n",
      "17800 35.6 0.4545 / 0 Correct\n",
      "17900 35.8 0.7427 / 1 Wrong\n",
      "18000 36.0 0.8471 / 0 Wrong\n",
      "18100 36.199999999999996 0.3336 / 0 Correct\n",
      "18200 36.4 1.3528 / 0 Wrong\n",
      "18300 36.6 1.3931 / 0 Wrong\n",
      "18400 36.8 1.4987 / 0 Wrong\n",
      "18500 37.0 0.7157 / 0 Wrong\n",
      "18600 37.2 0.6519 / 0 Correct\n",
      "18700 37.4 0.4815 / 0 Correct\n",
      "18800 37.6 0.3922 / 0 Correct\n",
      "18900 37.8 0.9938 / 0 Wrong\n",
      "19000 38.0 0.6118 / 0 Correct\n",
      "19100 38.2 0.5618 / 0 Correct\n",
      "19200 38.4 0.6903 / 1 Correct\n",
      "19300 38.6 0.8005 / 0 Wrong\n",
      "19400 38.800000000000004 0.6219 / 0 Correct\n",
      "19500 39.0 0.8906 / 1 Wrong\n",
      "19600 39.2 0.4653 / 0 Correct\n",
      "19700 39.4 1.4373 / 0 Wrong\n",
      "19800 39.6 0.7117 / 1 Wrong\n",
      "19900 39.800000000000004 0.4973 / 0 Correct\n",
      "20000 40.0 0.6249 / 0 Correct\n",
      "20100 40.2 0.4411 / 1 Correct\n",
      "20200 40.400000000000006 0.6112 / 1 Correct\n",
      "20300 40.6 0.6370 / 0 Correct\n",
      "20400 40.8 0.6669 / 1 Correct\n",
      "20500 41.0 0.3477 / 1 Correct\n",
      "20600 41.199999999999996 0.5486 / 1 Correct\n",
      "20700 41.4 0.6329 / 0 Correct\n",
      "20800 41.6 0.4649 / 0 Correct\n",
      "20900 41.8 0.2199 / 0 Correct\n",
      "21000 42.0 1.5692 / 0 Wrong\n",
      "21100 42.199999999999996 0.4111 / 0 Correct\n",
      "21200 42.4 0.9250 / 1 Wrong\n",
      "21300 42.6 0.4600 / 1 Correct\n",
      "21400 42.8 0.4281 / 1 Correct\n",
      "21500 43.0 0.7233 / 0 Wrong\n",
      "21600 43.2 1.1502 / 0 Wrong\n",
      "21700 43.4 0.4824 / 0 Correct\n",
      "21800 43.6 0.4335 / 0 Correct\n",
      "21900 43.8 0.7307 / 0 Wrong\n",
      "22000 44.0 0.4337 / 0 Correct\n",
      "22100 44.2 0.4294 / 1 Correct\n",
      "22200 44.4 0.3556 / 1 Correct\n",
      "22300 44.6 0.6199 / 1 Correct\n",
      "22400 44.800000000000004 0.7068 / 1 Wrong\n",
      "22500 45.0 0.7140 / 0 Wrong\n",
      "22600 45.2 1.0966 / 1 Wrong\n",
      "22700 45.4 0.2908 / 0 Correct\n",
      "22800 45.6 0.4954 / 1 Correct\n",
      "22900 45.800000000000004 0.3941 / 0 Correct\n",
      "23000 46.0 1.0547 / 0 Wrong\n",
      "23100 46.2 0.3774 / 0 Correct\n",
      "23200 46.400000000000006 1.0291 / 1 Wrong\n",
      "23300 46.6 0.7176 / 0 Wrong\n",
      "23400 46.800000000000004 0.6038 / 0 Correct\n",
      "23500 47.0 0.6919 / 0 Correct\n",
      "23600 47.199999999999996 1.2324 / 0 Wrong\n",
      "23700 47.4 0.5798 / 1 Correct\n",
      "23800 47.599999999999994 0.6739 / 0 Correct\n",
      "23900 47.8 0.3320 / 0 Correct\n",
      "24000 48.0 0.3570 / 0 Correct\n",
      "24100 48.199999999999996 0.4175 / 0 Correct\n",
      "24200 48.4 0.7391 / 1 Wrong\n",
      "24300 48.6 0.7276 / 1 Wrong\n",
      "24400 48.8 0.5946 / 0 Correct\n",
      "24500 49.0 0.6148 / 1 Correct\n",
      "24600 49.2 0.4246 / 0 Correct\n",
      "24700 49.4 0.3761 / 0 Correct\n",
      "24800 49.6 0.6199 / 1 Correct\n",
      "24900 49.8 0.4720 / 0 Correct\n",
      "25000 50.0 0.6579 / 0 Correct\n",
      "25100 50.2 0.3085 / 0 Correct\n",
      "25200 50.4 0.4128 / 0 Correct\n",
      "25300 50.6 0.6463 / 1 Correct\n",
      "25400 50.8 0.6912 / 1 Correct\n",
      "25500 51.0 0.2318 / 0 Correct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25600 51.2 0.4146 / 0 Correct\n",
      "25700 51.4 0.8925 / 0 Wrong\n",
      "25800 51.6 0.6954 / 1 Wrong\n",
      "25900 51.800000000000004 0.2398 / 0 Correct\n",
      "26000 52.0 0.3705 / 0 Correct\n",
      "26100 52.2 0.6860 / 0 Correct\n",
      "26200 52.400000000000006 0.6609 / 0 Correct\n",
      "26300 52.6 0.5961 / 0 Correct\n",
      "26400 52.800000000000004 0.3655 / 0 Correct\n",
      "26500 53.0 0.6121 / 1 Correct\n",
      "26600 53.2 1.1388 / 0 Wrong\n",
      "26700 53.400000000000006 0.4146 / 1 Correct\n",
      "26800 53.6 0.3428 / 0 Correct\n",
      "26900 53.800000000000004 0.5049 / 1 Correct\n",
      "27000 54.0 0.9690 / 0 Wrong\n",
      "27100 54.2 0.9552 / 0 Wrong\n",
      "27200 54.400000000000006 0.4940 / 0 Correct\n",
      "27300 54.6 0.2701 / 0 Correct\n",
      "27400 54.800000000000004 0.9564 / 0 Wrong\n",
      "27500 55.00000000000001 0.5470 / 1 Correct\n",
      "27600 55.2 0.7299 / 0 Wrong\n",
      "27700 55.400000000000006 0.3359 / 0 Correct\n",
      "27800 55.60000000000001 1.1436 / 0 Wrong\n",
      "27900 55.800000000000004 1.1112 / 0 Wrong\n",
      "28000 56.00000000000001 0.2250 / 0 Correct\n",
      "28100 56.2 1.0292 / 1 Wrong\n",
      "28200 56.39999999999999 0.4411 / 0 Correct\n",
      "28300 56.599999999999994 1.4926 / 1 Wrong\n",
      "28400 56.8 0.5398 / 0 Correct\n",
      "28500 56.99999999999999 0.3451 / 0 Correct\n",
      "28600 57.199999999999996 0.6868 / 1 Correct\n",
      "28700 57.4 0.4682 / 0 Correct\n",
      "28800 57.599999999999994 0.8734 / 1 Wrong\n",
      "28900 57.8 0.3447 / 0 Correct\n",
      "29000 57.99999999999999 0.7649 / 1 Wrong\n",
      "29100 58.199999999999996 0.9992 / 0 Wrong\n",
      "29200 58.4 1.1449 / 0 Wrong\n",
      "29300 58.599999999999994 0.6007 / 0 Correct\n",
      "29400 58.8 1.3662 / 0 Wrong\n",
      "29500 59.0 0.5124 / 1 Correct\n",
      "29600 59.199999999999996 0.4662 / 0 Correct\n",
      "29700 59.4 0.3201 / 0 Correct\n",
      "29800 59.599999999999994 0.7003 / 1 Wrong\n",
      "29900 59.8 0.4610 / 0 Correct\n",
      "30000 60.0 0.6543 / 1 Correct\n",
      "30100 60.199999999999996 0.7753 / 0 Wrong\n",
      "30200 60.4 0.0149 / 0 Correct\n",
      "30300 60.6 nan / 0 Wrong\n",
      "30400 60.8 nan / 0 Wrong\n",
      "30500 61.0 nan / 0 Correct\n",
      "30600 61.199999999999996 nan / 0 Correct\n",
      "30700 61.4 nan / 0 Correct\n",
      "30800 61.6 nan / 0 Correct\n",
      "30900 61.8 nan / 0 Correct\n",
      "31000 62.0 nan / 0 Correct\n",
      "31100 62.2 nan / 0 Correct\n",
      "31200 62.4 nan / 0 Wrong\n",
      "31300 62.6 nan / 0 Correct\n",
      "31400 62.8 nan / 0 Correct\n",
      "31500 63.0 nan / 0 Correct\n",
      "31600 63.2 nan / 0 Correct\n",
      "31700 63.4 nan / 0 Wrong\n",
      "31800 63.6 nan / 0 Wrong\n",
      "31900 63.800000000000004 nan / 0 Wrong\n",
      "32000 64.0 nan / 0 Wrong\n",
      "32100 64.2 nan / 0 Wrong\n",
      "32200 64.4 nan / 0 Correct\n",
      "32300 64.60000000000001 nan / 0 Wrong\n",
      "32400 64.8 nan / 0 Wrong\n",
      "32500 65.0 nan / 0 Correct\n",
      "32600 65.2 nan / 0 Correct\n",
      "32700 65.4 nan / 0 Correct\n",
      "32800 65.60000000000001 nan / 0 Correct\n",
      "32900 65.8 nan / 0 Correct\n",
      "33000 66.0 nan / 0 Correct\n",
      "33100 66.2 nan / 0 Correct\n",
      "33200 66.4 nan / 0 Correct\n",
      "33300 66.60000000000001 nan / 0 Correct\n",
      "33400 66.8 nan / 0 Correct\n",
      "33500 67.0 nan / 0 Wrong\n",
      "33600 67.2 nan / 0 Wrong\n",
      "33700 67.4 nan / 0 Wrong\n",
      "33800 67.60000000000001 nan / 0 Correct\n",
      "33900 67.80000000000001 nan / 0 Correct\n",
      "34000 68.0 nan / 0 Wrong\n",
      "34100 68.2 nan / 0 Wrong\n",
      "34200 68.4 nan / 0 Correct\n",
      "34300 68.60000000000001 nan / 0 Correct\n",
      "34400 68.8 nan / 0 Wrong\n",
      "34500 69.0 nan / 0 Correct\n",
      "34600 69.19999999999999 nan / 0 Correct\n",
      "34700 69.39999999999999 nan / 0 Correct\n",
      "34800 69.6 nan / 0 Correct\n",
      "34900 69.8 nan / 0 Wrong\n",
      "35000 70.0 nan / 0 Correct\n",
      "35100 70.19999999999999 nan / 0 Correct\n",
      "35200 70.39999999999999 nan / 0 Wrong\n",
      "35300 70.6 nan / 0 Wrong\n",
      "35400 70.8 nan / 0 Correct\n",
      "35500 71.0 nan / 0 Wrong\n",
      "35600 71.2 nan / 0 Correct\n",
      "35700 71.39999999999999 nan / 0 Correct\n",
      "35800 71.6 nan / 0 Correct\n",
      "35900 71.8 nan / 0 Wrong\n",
      "36000 72.0 nan / 0 Correct\n",
      "36100 72.2 nan / 0 Correct\n",
      "36200 72.39999999999999 nan / 0 Wrong\n",
      "36300 72.6 nan / 0 Correct\n",
      "36400 72.8 nan / 0 Correct\n",
      "36500 73.0 nan / 0 Correct\n",
      "36600 73.2 nan / 0 Wrong\n",
      "36700 73.4 nan / 0 Wrong\n",
      "36800 73.6 nan / 0 Correct\n",
      "36900 73.8 nan / 0 Correct\n",
      "37000 74.0 nan / 0 Wrong\n",
      "37100 74.2 nan / 0 Correct\n",
      "37200 74.4 nan / 0 Correct\n",
      "37300 74.6 nan / 0 Correct\n",
      "37400 74.8 nan / 0 Correct\n",
      "37500 75.0 nan / 0 Correct\n",
      "37600 75.2 nan / 0 Wrong\n",
      "37700 75.4 nan / 0 Correct\n",
      "37800 75.6 nan / 0 Correct\n",
      "37900 75.8 nan / 0 Correct\n",
      "38000 76.0 nan / 0 Wrong\n",
      "38100 76.2 nan / 0 Wrong\n",
      "38200 76.4 nan / 0 Correct\n",
      "38300 76.6 nan / 0 Wrong\n",
      "38400 76.8 nan / 0 Wrong\n",
      "38500 77.0 nan / 0 Correct\n",
      "38600 77.2 nan / 0 Correct\n",
      "38700 77.4 nan / 0 Wrong\n",
      "38800 77.60000000000001 nan / 0 Correct\n",
      "38900 77.8 nan / 0 Correct\n",
      "39000 78.0 nan / 0 Correct\n",
      "39100 78.2 nan / 0 Wrong\n",
      "39200 78.4 nan / 0 Correct\n",
      "39300 78.60000000000001 nan / 0 Wrong\n",
      "39400 78.8 nan / 0 Correct\n",
      "39500 79.0 nan / 0 Correct\n",
      "39600 79.2 nan / 0 Wrong\n",
      "39700 79.4 nan / 0 Wrong\n",
      "39800 79.60000000000001 nan / 0 Correct\n",
      "39900 79.80000000000001 nan / 0 Correct\n",
      "40000 80.0 nan / 0 Wrong\n",
      "40100 80.2 nan / 0 Correct\n",
      "40200 80.4 nan / 0 Correct\n",
      "40300 80.60000000000001 nan / 0 Correct\n",
      "40400 80.80000000000001 nan / 0 Correct\n",
      "40500 81.0 nan / 0 Wrong\n",
      "40600 81.2 nan / 0 Correct\n",
      "40700 81.39999999999999 nan / 0 Correct\n",
      "40800 81.6 nan / 0 Correct\n",
      "40900 81.8 nan / 0 Correct\n",
      "41000 82.0 nan / 0 Wrong\n",
      "41100 82.19999999999999 nan / 0 Wrong\n",
      "41200 82.39999999999999 nan / 0 Wrong\n",
      "41300 82.6 nan / 0 Wrong\n",
      "41400 82.8 nan / 0 Correct\n",
      "41500 83.0 nan / 0 Wrong\n",
      "41600 83.2 nan / 0 Wrong\n",
      "41700 83.39999999999999 nan / 0 Correct\n",
      "41800 83.6 nan / 0 Correct\n",
      "41900 83.8 nan / 0 Correct\n",
      "42000 84.0 nan / 0 Correct\n",
      "42100 84.2 nan / 0 Correct\n",
      "42200 84.39999999999999 nan / 0 Correct\n",
      "42300 84.6 nan / 0 Correct\n",
      "42400 84.8 nan / 0 Wrong\n",
      "42500 85.0 nan / 0 Correct\n",
      "42600 85.2 nan / 0 Correct\n",
      "42700 85.39999999999999 nan / 0 Correct\n",
      "42800 85.6 nan / 0 Wrong\n",
      "42900 85.8 nan / 0 Correct\n",
      "43000 86.0 nan / 0 Correct\n",
      "43100 86.2 nan / 0 Wrong\n",
      "43200 86.4 nan / 0 Correct\n",
      "43300 86.6 nan / 0 Wrong\n",
      "43400 86.8 nan / 0 Correct\n",
      "43500 87.0 nan / 0 Correct\n",
      "43600 87.2 nan / 0 Correct\n",
      "43700 87.4 nan / 0 Wrong\n",
      "43800 87.6 nan / 0 Correct\n",
      "43900 87.8 nan / 0 Wrong\n",
      "44000 88.0 nan / 0 Wrong\n",
      "44100 88.2 nan / 0 Wrong\n",
      "44200 88.4 nan / 0 Correct\n",
      "44300 88.6 nan / 0 Wrong\n",
      "44400 88.8 nan / 0 Correct\n",
      "44500 89.0 nan / 0 Wrong\n",
      "44600 89.2 nan / 0 Wrong\n",
      "44700 89.4 nan / 0 Wrong\n",
      "44800 89.60000000000001 nan / 0 Wrong\n",
      "44900 89.8 nan / 0 Wrong\n",
      "45000 90.0 nan / 0 Correct\n",
      "45100 90.2 nan / 0 Correct\n",
      "45200 90.4 nan / 0 Wrong\n",
      "45300 90.60000000000001 nan / 0 Correct\n",
      "45400 90.8 nan / 0 Wrong\n",
      "45500 91.0 nan / 0 Correct\n",
      "45600 91.2 nan / 0 Wrong\n",
      "45700 91.4 nan / 0 Correct\n",
      "45800 91.60000000000001 nan / 0 Wrong\n",
      "45900 91.8 nan / 0 Correct\n",
      "46000 92.0 nan / 0 Wrong\n",
      "46100 92.2 nan / 0 Correct\n",
      "46200 92.4 nan / 0 Correct\n",
      "46300 92.60000000000001 nan / 0 Wrong\n",
      "46400 92.80000000000001 nan / 0 Correct\n",
      "46500 93.0 nan / 0 Wrong\n",
      "46600 93.2 nan / 0 Wrong\n",
      "46700 93.4 nan / 0 Wrong\n",
      "46800 93.60000000000001 nan / 0 Wrong\n",
      "46900 93.8 nan / 0 Correct\n",
      "47000 94.0 nan / 0 Wrong\n",
      "47100 94.19999999999999 nan / 0 Correct\n",
      "47200 94.39999999999999 nan / 0 Correct\n",
      "47300 94.6 nan / 0 Correct\n",
      "47400 94.8 nan / 0 Wrong\n",
      "47500 95.0 nan / 0 Wrong\n",
      "47600 95.19999999999999 nan / 0 Correct\n",
      "47700 95.39999999999999 nan / 0 Correct\n",
      "47800 95.6 nan / 0 Correct\n",
      "47900 95.8 nan / 0 Correct\n",
      "48000 96.0 nan / 0 Correct\n",
      "48100 96.2 nan / 0 Correct\n",
      "48200 96.39999999999999 nan / 0 Correct\n",
      "48300 96.6 nan / 0 Correct\n",
      "48400 96.8 nan / 0 Wrong\n",
      "48500 97.0 nan / 0 Wrong\n",
      "48600 97.2 nan / 0 Wrong\n",
      "48700 97.39999999999999 nan / 0 Correct\n",
      "48800 97.6 nan / 0 Wrong\n",
      "48900 97.8 nan / 0 Correct\n",
      "49000 98.0 nan / 0 Wrong\n",
      "49100 98.2 nan / 0 Correct\n",
      "49200 98.4 nan / 0 Wrong\n",
      "49300 98.6 nan / 0 Correct\n",
      "49400 98.8 nan / 0 Correct\n",
      "49500 99.0 nan / 0 Correct\n",
      "49600 99.2 nan / 0 Correct\n",
      "49700 99.4 nan / 0 Correct\n",
      "49800 99.6 nan / 0 Wrong\n",
      "49900 99.8 nan / 0 Correct\n",
      "50000 100.0 nan / 0 Correct\n"
     ]
    }
   ],
   "source": [
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "plot_steps, print_steps = 1000, 100\n",
    "n_iters = 50000\n",
    "for i in range(n_iters):\n",
    "    line_tensor, target  = random_training_exmple(train_df)\n",
    "    output, loss = train(line_tensor, torch.tensor([target]))\n",
    "#     print(output) if str(loss) == \"nan\" else True\n",
    "    current_loss += loss\n",
    "    if (i+1) % plot_steps == 0:\n",
    "        all_losses.append(current_loss/ plot_steps)\n",
    "        current_loss = 0\n",
    "        \n",
    "    if (i+1) % print_steps == 0:\n",
    "        guess = category_from_output(output)\n",
    "        correct = \"Correct\" if guess == target else f\"Wrong\"\n",
    "        print(f\"{i+1} {(i+1)/n_iters*100} {loss:.4f} / {guess} {correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fb300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.show()\n",
    "while True:\n",
    "    sentence = input(\"Input:\")\n",
    "    if sentence ==\"Quit\":\n",
    "        break\n",
    "    predict(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
